{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "      <th>Cultivar 1</th>\n",
       "      <th>Cultivar 2</th>\n",
       "      <th>Cultivar 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.518613</td>\n",
       "      <td>-0.562250</td>\n",
       "      <td>0.232053</td>\n",
       "      <td>-1.169593</td>\n",
       "      <td>1.913905</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>1.034819</td>\n",
       "      <td>-0.659563</td>\n",
       "      <td>1.224884</td>\n",
       "      <td>0.251717</td>\n",
       "      <td>0.362177</td>\n",
       "      <td>1.847920</td>\n",
       "      <td>1.013009</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.246290</td>\n",
       "      <td>-0.499413</td>\n",
       "      <td>-0.827996</td>\n",
       "      <td>-2.490847</td>\n",
       "      <td>0.018145</td>\n",
       "      <td>0.568648</td>\n",
       "      <td>0.733629</td>\n",
       "      <td>-0.820719</td>\n",
       "      <td>-0.544721</td>\n",
       "      <td>-0.293321</td>\n",
       "      <td>0.406051</td>\n",
       "      <td>1.113449</td>\n",
       "      <td>0.965242</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.196879</td>\n",
       "      <td>0.021231</td>\n",
       "      <td>1.109334</td>\n",
       "      <td>-0.268738</td>\n",
       "      <td>0.088358</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>1.215533</td>\n",
       "      <td>-0.498407</td>\n",
       "      <td>2.135968</td>\n",
       "      <td>0.269020</td>\n",
       "      <td>0.318304</td>\n",
       "      <td>0.788587</td>\n",
       "      <td>1.395148</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.691550</td>\n",
       "      <td>-0.346811</td>\n",
       "      <td>0.487926</td>\n",
       "      <td>-0.809251</td>\n",
       "      <td>0.930918</td>\n",
       "      <td>2.491446</td>\n",
       "      <td>1.466525</td>\n",
       "      <td>-0.981875</td>\n",
       "      <td>1.032155</td>\n",
       "      <td>1.186068</td>\n",
       "      <td>-0.427544</td>\n",
       "      <td>1.184071</td>\n",
       "      <td>2.334574</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.295700</td>\n",
       "      <td>0.227694</td>\n",
       "      <td>1.840403</td>\n",
       "      <td>0.451946</td>\n",
       "      <td>1.281985</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>0.663351</td>\n",
       "      <td>0.226796</td>\n",
       "      <td>0.401404</td>\n",
       "      <td>-0.319276</td>\n",
       "      <td>0.362177</td>\n",
       "      <td>0.449601</td>\n",
       "      <td>-0.037874</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Alcohol  Malic acid       Ash  Alcalinity of ash  Magnesium  \\\n",
       "0  1.518613   -0.562250  0.232053          -1.169593   1.913905   \n",
       "1  0.246290   -0.499413 -0.827996          -2.490847   0.018145   \n",
       "2  0.196879    0.021231  1.109334          -0.268738   0.088358   \n",
       "3  1.691550   -0.346811  0.487926          -0.809251   0.930918   \n",
       "4  0.295700    0.227694  1.840403           0.451946   1.281985   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0       0.808997    1.034819             -0.659563         1.224884   \n",
       "1       0.568648    0.733629             -0.820719        -0.544721   \n",
       "2       0.808997    1.215533             -0.498407         2.135968   \n",
       "3       2.491446    1.466525             -0.981875         1.032155   \n",
       "4       0.808997    0.663351              0.226796         0.401404   \n",
       "\n",
       "   Color intensity       Hue  OD280/OD315 of diluted wines   Proline  \\\n",
       "0         0.251717  0.362177                      1.847920  1.013009   \n",
       "1        -0.293321  0.406051                      1.113449  0.965242   \n",
       "2         0.269020  0.318304                      0.788587  1.395148   \n",
       "3         1.186068 -0.427544                      1.184071  2.334574   \n",
       "4        -0.319276  0.362177                      0.449601 -0.037874   \n",
       "\n",
       "   Cultivar 1  Cultivar 2  Cultivar 3  \n",
       "0           1           0           0  \n",
       "1           1           0           0  \n",
       "2           1           0           0  \n",
       "3           1           0           0  \n",
       "4           1           0           0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('W1data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels\n",
    "y = df[['Cultivar 1', 'Cultivar 2', 'Cultivar 3']].values\n",
    "\n",
    "# Get inputs; we define our x and y here.\n",
    "X = df.drop(['Cultivar 1', 'Cultivar 2', 'Cultivar 3'], axis = 1)\n",
    "X.shape, y.shape # Print shapes just to check\n",
    "X = X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define all our functions\n",
    "\n",
    "def softmax(z):\n",
    "    #Calculate exponent term first\n",
    "    exp_scores = np.exp(z)\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(y,y_hat):\n",
    "    # Clipping value\n",
    "    minval = 0.000000000001\n",
    "    # Number of samples\n",
    "    m = y.shape[0]\n",
    "    # Loss formula, note that np.sum sums up the entire matrix and therefore does the job of two sums from the formula\n",
    "    loss = -1/m * np.sum(y * np.log(y_hat.clip(min=minval)))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_derivative(y,y_hat):\n",
    "    return (y_hat-y)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 - np.power(x, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each input is multiplied by the dot product of all the random initialised \n",
    "#weights + a bias which starts at 0 but will increaseduring trianing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the forward propagation function\n",
    "def forward_prop(model,a0):\n",
    "    \n",
    "    #Start Forward Propagation\n",
    "    \n",
    "    # Load parameters from model\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3']\n",
    "    \n",
    "    # Do the first Linear step \n",
    "    # Z1 is the input layer x times the dot product of the weights + our bias b\n",
    "    z1 = a0.dot(W1) + b1\n",
    "    \n",
    "    # Put it through the first activation function\n",
    "    a1 = np.tanh(z1)\n",
    "    \n",
    "    # Second linear step\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    \n",
    "    # Second activation function\n",
    "    a2 = np.tanh(z2)\n",
    "    \n",
    "    #Third linear step\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    \n",
    "    #For the Third linear activation function we use the softmax function, either the sigmoid of softmax should be used for the last layer\n",
    "    a3 = softmax(z3)\n",
    "    \n",
    "    #Store all results in these values\n",
    "    cache = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3}\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the BACKWARD PROPAGATION function\n",
    "def backward_prop(model,cache,y):\n",
    "\n",
    "    # Load parameters from model\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'],model['W3'],model['b3']\n",
    "    \n",
    "    # Load forward propagation results\n",
    "    a0,a1, a2,a3 = cache['a0'],cache['a1'],cache['a2'],cache['a3']\n",
    "    \n",
    "    # Get number of samples\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    # Calculate loss derivative with respect to output\n",
    "    dz3 = loss_derivative(y=y,y_hat=a3)\n",
    "\n",
    "    # Calculate loss derivative with respect to second layer weights\n",
    "    dW3 = 1/m*(a2.T).dot(dz3) #dW2 = 1/m*(a1.T).dot(dz2) \n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer bias\n",
    "    db3 = 1/m*np.sum(dz3, axis=0)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer\n",
    "    dz2 = np.multiply(dz3.dot(W3.T) ,tanh_derivative(a2))\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer weights\n",
    "    dW2 = 1/m*np.dot(a1.T, dz2)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer bias\n",
    "    db2 = 1/m*np.sum(dz2, axis=0)\n",
    "    \n",
    "    dz1 = np.multiply(dz2.dot(W2.T),tanh_derivative(a1))\n",
    "    \n",
    "    dW1 = 1/m*np.dot(a0.T,dz1)\n",
    "    \n",
    "    db1 = 1/m*np.sum(dz1,axis=0)\n",
    "    \n",
    "    # Store gradients\n",
    "    grads = {'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING PHASE\n",
    "def initialise_parameters(nn_input_dim,nn_hdim,nn_output_dim):\n",
    "    # First layer weights\n",
    "    W1 = 2 *np.random.randn(nn_input_dim, nn_hdim) - 1\n",
    "    \n",
    "    # First layer bias\n",
    "    b1 = np.zeros((1, nn_hdim))\n",
    "    \n",
    "    # Second layer weights\n",
    "    W2 = 2 * np.random.randn(nn_hdim, nn_hdim) - 1\n",
    "    \n",
    "    # Second layer bias\n",
    "    b2 = np.zeros((1, nn_hdim))\n",
    "    W3 = 2 * np.random.rand(nn_hdim, nn_output_dim) - 1\n",
    "    b3 = np.zeros((1,nn_output_dim))\n",
    "    \n",
    "    \n",
    "    # Package and return model\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(model,grads,learning_rate):\n",
    "    # Load parameters\n",
    "    W1, b1, W2, b2,b3,W3 = model['W1'], model['b1'], model['W2'], model['b2'],model['b3'],model[\"W3\"]\n",
    "    \n",
    "    # Update parameters\n",
    "    W1 -= learning_rate * grads['dW1']\n",
    "    b1 -= learning_rate * grads['db1']\n",
    "    W2 -= learning_rate * grads['dW2']\n",
    "    b2 -= learning_rate * grads['db2']\n",
    "    W3 -= learning_rate * grads['dW3']\n",
    "    b3 -= learning_rate * grads['db3']\n",
    "    \n",
    "    # Store and return parameters\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3':W3,'b3':b3}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x):\n",
    "    # Do forward pass\n",
    "    c = forward_prop(model,x)\n",
    "    #get y_hat\n",
    "    y_hat = np.argmax(c['a3'], axis=1)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(model,x,y):\n",
    "    # Get total number of examples\n",
    "    m = y.shape[0]\n",
    "    # Do a prediction with the model\n",
    "    pred = predict(model,x)\n",
    "    # Ensure prediction and truth vector y have the same shape\n",
    "    pred = pred.reshape(y.shape)\n",
    "    # Calculate the number of wrong examples\n",
    "    error = np.sum(np.abs(pred-y))\n",
    "    # Calculate accuracy\n",
    "    return (m - error)/m * 100\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,X_,y_,learning_rate, epochs=20000, print_loss=False):\n",
    "    # Gradient descent. Loop over epochs\n",
    "    for i in range(0, epochs):\n",
    "\n",
    "        # Forward propagation\n",
    "        cache = forward_prop(model,X_)\n",
    "        #a1, probs = cache['a1'],cache['a2']\n",
    "        # Backpropagation\n",
    "        \n",
    "        grads = backward_prop(model,cache,y_)\n",
    "        # Gradient descent parameter update\n",
    "        # Assign new parameters to the model\n",
    "        model = update_parameters(model=model,grads=grads,learning_rate=learning_rate)\n",
    "    \n",
    "        # Pring loss & accuracy every 100 iterations\n",
    "        if print_loss and i % 100 == 0:\n",
    "            a3 = cache['a3']\n",
    "            print('Loss after iteration',i,':',softmax_loss(y_,a3))\n",
    "            y_hat = predict(model,X_)\n",
    "            y_true = y_.argmax(axis=1)\n",
    "            print('Accuracy after iteration',i,':',accuracy_score(y_pred=y_hat,y_true=y_true)*100,'%')\n",
    "            losses.append(accuracy_score(y_pred=y_hat,y_true=y_true)*100)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to make random start from a base of 0\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0 : 1.56620214144\n",
      "Accuracy after iteration 0 : 26.9662921348 %\n",
      "Loss after iteration 100 : 0.487012611166\n",
      "Accuracy after iteration 100 : 81.4606741573 %\n",
      "Loss after iteration 200 : 0.410918015699\n",
      "Accuracy after iteration 200 : 83.7078651685 %\n",
      "Loss after iteration 300 : 0.351194258123\n",
      "Accuracy after iteration 300 : 83.7078651685 %\n",
      "Loss after iteration 400 : 0.305606335087\n",
      "Accuracy after iteration 400 : 85.9550561798 %\n",
      "Loss after iteration 500 : 0.248099403624\n",
      "Accuracy after iteration 500 : 87.6404494382 %\n",
      "Loss after iteration 600 : 0.227772600029\n",
      "Accuracy after iteration 600 : 88.202247191 %\n",
      "Loss after iteration 700 : 0.20044246943\n",
      "Accuracy after iteration 700 : 89.3258426966 %\n",
      "Loss after iteration 800 : 0.188029879087\n",
      "Accuracy after iteration 800 : 92.1348314607 %\n",
      "Loss after iteration 900 : 0.176999106487\n",
      "Accuracy after iteration 900 : 92.6966292135 %\n",
      "Loss after iteration 1000 : 0.170865544109\n",
      "Accuracy after iteration 1000 : 92.1348314607 %\n",
      "Loss after iteration 1100 : 0.166045535748\n",
      "Accuracy after iteration 1100 : 92.6966292135 %\n",
      "Loss after iteration 1200 : 0.159818201917\n",
      "Accuracy after iteration 1200 : 93.8202247191 %\n",
      "Loss after iteration 1300 : 0.153135585697\n",
      "Accuracy after iteration 1300 : 93.8202247191 %\n",
      "Loss after iteration 1400 : 0.144934153412\n",
      "Accuracy after iteration 1400 : 93.8202247191 %\n",
      "Loss after iteration 1500 : 0.139871384993\n",
      "Accuracy after iteration 1500 : 94.3820224719 %\n",
      "Loss after iteration 1600 : 0.135304193928\n",
      "Accuracy after iteration 1600 : 94.9438202247 %\n",
      "Loss after iteration 1700 : 0.131363209289\n",
      "Accuracy after iteration 1700 : 94.9438202247 %\n",
      "Loss after iteration 1800 : 0.127034848235\n",
      "Accuracy after iteration 1800 : 94.9438202247 %\n",
      "Loss after iteration 1900 : 0.120597235047\n",
      "Accuracy after iteration 1900 : 96.0674157303 %\n",
      "Loss after iteration 2000 : 0.110538648784\n",
      "Accuracy after iteration 2000 : 96.6292134831 %\n",
      "Loss after iteration 2100 : 0.0977223805114\n",
      "Accuracy after iteration 2100 : 97.191011236 %\n",
      "Loss after iteration 2200 : 0.0825874018493\n",
      "Accuracy after iteration 2200 : 97.7528089888 %\n",
      "Loss after iteration 2300 : 0.0764682411527\n",
      "Accuracy after iteration 2300 : 97.7528089888 %\n",
      "Loss after iteration 2400 : 0.0701115616064\n",
      "Accuracy after iteration 2400 : 98.3146067416 %\n",
      "Loss after iteration 2500 : 0.0585272550866\n",
      "Accuracy after iteration 2500 : 98.3146067416 %\n",
      "Loss after iteration 2600 : 0.0488064387624\n",
      "Accuracy after iteration 2600 : 98.8764044944 %\n",
      "Loss after iteration 2700 : 0.0466907842048\n",
      "Accuracy after iteration 2700 : 98.8764044944 %\n",
      "Loss after iteration 2800 : 0.0453737539926\n",
      "Accuracy after iteration 2800 : 98.8764044944 %\n",
      "Loss after iteration 2900 : 0.0444977064758\n",
      "Accuracy after iteration 2900 : 98.8764044944 %\n",
      "Loss after iteration 3000 : 0.0438368046833\n",
      "Accuracy after iteration 3000 : 98.8764044944 %\n",
      "Loss after iteration 3100 : 0.0433007048004\n",
      "Accuracy after iteration 3100 : 98.8764044944 %\n",
      "Loss after iteration 3200 : 0.0428487944771\n",
      "Accuracy after iteration 3200 : 98.8764044944 %\n",
      "Loss after iteration 3300 : 0.0424577619924\n",
      "Accuracy after iteration 3300 : 98.8764044944 %\n",
      "Loss after iteration 3400 : 0.0421127850037\n",
      "Accuracy after iteration 3400 : 98.8764044944 %\n",
      "Loss after iteration 3500 : 0.0418038406128\n",
      "Accuracy after iteration 3500 : 98.8764044944 %\n",
      "Loss after iteration 3600 : 0.0415238400244\n",
      "Accuracy after iteration 3600 : 98.8764044944 %\n",
      "Loss after iteration 3700 : 0.0412675838515\n",
      "Accuracy after iteration 3700 : 98.8764044944 %\n",
      "Loss after iteration 3800 : 0.0410311372781\n",
      "Accuracy after iteration 3800 : 98.8764044944 %\n",
      "Loss after iteration 3900 : 0.0408114382808\n",
      "Accuracy after iteration 3900 : 98.8764044944 %\n",
      "Loss after iteration 4000 : 0.0406060426878\n",
      "Accuracy after iteration 4000 : 98.8764044944 %\n",
      "Loss after iteration 4100 : 0.0404129529526\n",
      "Accuracy after iteration 4100 : 98.8764044944 %\n",
      "Loss after iteration 4200 : 0.0402304998017\n",
      "Accuracy after iteration 4200 : 98.3146067416 %\n",
      "Loss after iteration 4300 : 0.0400572581367\n",
      "Accuracy after iteration 4300 : 98.3146067416 %\n",
      "Loss after iteration 4400 : 0.0398919855927\n",
      "Accuracy after iteration 4400 : 98.3146067416 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x104fb5d30>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#epochs is one full training forward and back propagation\n",
    "#LEarning rate is hoow rapidly the parameters change\n",
    "# This is what we return at the end\n",
    "model = initialise_parameters(nn_input_dim=13, nn_hdim= 5, nn_output_dim= 3)\n",
    "model = train(model,X,y,learning_rate=0.07,epochs=4500,print_loss=True)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
