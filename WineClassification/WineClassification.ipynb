{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "      <th>Cultivar 1</th>\n",
       "      <th>Cultivar 2</th>\n",
       "      <th>Cultivar 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.246290</td>\n",
       "      <td>-0.499413</td>\n",
       "      <td>-0.827996</td>\n",
       "      <td>-2.490847</td>\n",
       "      <td>0.018145</td>\n",
       "      <td>0.568648</td>\n",
       "      <td>0.733629</td>\n",
       "      <td>-0.820719</td>\n",
       "      <td>-0.544721</td>\n",
       "      <td>-0.293321</td>\n",
       "      <td>0.406051</td>\n",
       "      <td>1.113449</td>\n",
       "      <td>0.965242</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.691550</td>\n",
       "      <td>-0.346811</td>\n",
       "      <td>0.487926</td>\n",
       "      <td>-0.809251</td>\n",
       "      <td>0.930918</td>\n",
       "      <td>2.491446</td>\n",
       "      <td>1.466525</td>\n",
       "      <td>-0.981875</td>\n",
       "      <td>1.032155</td>\n",
       "      <td>1.186068</td>\n",
       "      <td>-0.427544</td>\n",
       "      <td>1.184071</td>\n",
       "      <td>2.334574</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.295700</td>\n",
       "      <td>0.227694</td>\n",
       "      <td>1.840403</td>\n",
       "      <td>0.451946</td>\n",
       "      <td>1.281985</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>0.663351</td>\n",
       "      <td>0.226796</td>\n",
       "      <td>0.401404</td>\n",
       "      <td>-0.319276</td>\n",
       "      <td>0.362177</td>\n",
       "      <td>0.449601</td>\n",
       "      <td>-0.037874</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.481555</td>\n",
       "      <td>-0.517367</td>\n",
       "      <td>0.305159</td>\n",
       "      <td>-1.289707</td>\n",
       "      <td>0.860705</td>\n",
       "      <td>1.562093</td>\n",
       "      <td>1.366128</td>\n",
       "      <td>-0.176095</td>\n",
       "      <td>0.664217</td>\n",
       "      <td>0.731870</td>\n",
       "      <td>0.406051</td>\n",
       "      <td>0.336606</td>\n",
       "      <td>2.239039</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.716255</td>\n",
       "      <td>-0.418624</td>\n",
       "      <td>0.305159</td>\n",
       "      <td>-1.469878</td>\n",
       "      <td>-0.262708</td>\n",
       "      <td>0.328298</td>\n",
       "      <td>0.492677</td>\n",
       "      <td>-0.498407</td>\n",
       "      <td>0.681738</td>\n",
       "      <td>0.083015</td>\n",
       "      <td>0.274431</td>\n",
       "      <td>1.367689</td>\n",
       "      <td>1.729520</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Alcohol  Malic acid       Ash  Alcalinity of ash  Magnesium  \\\n",
       "0  0.246290   -0.499413 -0.827996          -2.490847   0.018145   \n",
       "1  1.691550   -0.346811  0.487926          -0.809251   0.930918   \n",
       "2  0.295700    0.227694  1.840403           0.451946   1.281985   \n",
       "3  1.481555   -0.517367  0.305159          -1.289707   0.860705   \n",
       "4  1.716255   -0.418624  0.305159          -1.469878  -0.262708   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0       0.568648    0.733629             -0.820719        -0.544721   \n",
       "1       2.491446    1.466525             -0.981875         1.032155   \n",
       "2       0.808997    0.663351              0.226796         0.401404   \n",
       "3       1.562093    1.366128             -0.176095         0.664217   \n",
       "4       0.328298    0.492677             -0.498407         0.681738   \n",
       "\n",
       "   Color intensity       Hue  OD280/OD315 of diluted wines   Proline  \\\n",
       "0        -0.293321  0.406051                      1.113449  0.965242   \n",
       "1         1.186068 -0.427544                      1.184071  2.334574   \n",
       "2        -0.319276  0.362177                      0.449601 -0.037874   \n",
       "3         0.731870  0.406051                      0.336606  2.239039   \n",
       "4         0.083015  0.274431                      1.367689  1.729520   \n",
       "\n",
       "   Cultivar 1  Cultivar 2  Cultivar 3  \n",
       "0           1           0           0  \n",
       "1           1           0           0  \n",
       "2           1           0           0  \n",
       "3           1           0           0  \n",
       "4           1           0           0  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('W1data.csv')\n",
    "dfTest = pd.read_csv('W1TestData.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels\n",
    "y = df[['Cultivar 1', 'Cultivar 2', 'Cultivar 3']].values\n",
    "\n",
    "# Get inputs; we define our x and y here.\n",
    "X = df.drop(['Cultivar 1', 'Cultivar 2', 'Cultivar 3'], axis = 1)\n",
    "X.shape, y.shape # Print shapes just to check\n",
    "X = X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define all our functions\n",
    "\n",
    "def softmax(z):\n",
    "    #Calculate exponent term first\n",
    "    exp_scores = np.exp(z)\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(y,y_hat):\n",
    "    # Clipping value\n",
    "    minval = 0.000000000001\n",
    "    # Number of samples\n",
    "    m = y.shape[0]\n",
    "    # Loss formula, note that np.sum sums up the entire matrix and therefore does the job of two sums from the formula\n",
    "    loss = -1/m * np.sum(y * np.log(y_hat.clip(min=minval)))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_derivative(y,y_hat):\n",
    "    return (y_hat-y)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 - np.power(x, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each input is multiplied by the dot product of all the random initialised \n",
    "#weights + a bias which starts at 0 but will increaseduring trianing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the forward propagation function\n",
    "def forward_prop(model,a0):\n",
    "    \n",
    "    #Start Forward Propagation\n",
    "    \n",
    "    # Load parameters from model\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3']\n",
    "    \n",
    "    # Do the first Linear step \n",
    "    # Z1 is the input layer x times the dot product of the weights + our bias b\n",
    "    z1 = a0.dot(W1) + b1\n",
    "    \n",
    "    # Put it through the first activation function\n",
    "    a1 = np.tanh(z1)\n",
    "    \n",
    "    # Second linear step\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    \n",
    "    # Second activation function\n",
    "    a2 = np.tanh(z2)\n",
    "    \n",
    "    #Third linear step\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    \n",
    "    #For the Third linear activation function we use the softmax function, either the sigmoid of softmax should be used for the last layer\n",
    "    a3 = softmax(z3)\n",
    "    \n",
    "    #Store all results in these values\n",
    "    cache = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3}\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the BACKWARD PROPAGATION function\n",
    "def backward_prop(model,cache,y):\n",
    "\n",
    "    # Load parameters from model\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'],model['W3'],model['b3']\n",
    "    \n",
    "    # Load forward propagation results\n",
    "    a0,a1, a2,a3 = cache['a0'],cache['a1'],cache['a2'],cache['a3']\n",
    "    \n",
    "    # Get number of samples\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    # Calculate loss derivative with respect to output\n",
    "    dz3 = loss_derivative(y=y,y_hat=a3)\n",
    "\n",
    "    # Calculate loss derivative with respect to second layer weights\n",
    "    dW3 = 1/m*(a2.T).dot(dz3) #dW2 = 1/m*(a1.T).dot(dz2) \n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer bias\n",
    "    db3 = 1/m*np.sum(dz3, axis=0)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer\n",
    "    dz2 = np.multiply(dz3.dot(W3.T) ,tanh_derivative(a2))\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer weights\n",
    "    dW2 = 1/m*np.dot(a1.T, dz2)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer bias\n",
    "    db2 = 1/m*np.sum(dz2, axis=0)\n",
    "    \n",
    "    dz1 = np.multiply(dz2.dot(W2.T),tanh_derivative(a1))\n",
    "    \n",
    "    dW1 = 1/m*np.dot(a0.T,dz1)\n",
    "    \n",
    "    db1 = 1/m*np.sum(dz1,axis=0)\n",
    "    \n",
    "    # Store gradients\n",
    "    grads = {'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING PHASE\n",
    "def initialise_parameters(nn_input_dim,nn_hdim,nn_output_dim):\n",
    "    # First layer weights\n",
    "    W1 = 2 *np.random.randn(nn_input_dim, nn_hdim) - 1\n",
    "    \n",
    "    # First layer bias\n",
    "    b1 = np.zeros((1, nn_hdim))\n",
    "    \n",
    "    # Second layer weights\n",
    "    W2 = 2 * np.random.randn(nn_hdim, nn_hdim) - 1\n",
    "    \n",
    "    # Second layer bias\n",
    "    b2 = np.zeros((1, nn_hdim))\n",
    "    W3 = 2 * np.random.rand(nn_hdim, nn_output_dim) - 1\n",
    "    b3 = np.zeros((1,nn_output_dim))\n",
    "    \n",
    "    \n",
    "    # Package and return model\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(model,grads,learning_rate):\n",
    "    # Load parameters\n",
    "    W1, b1, W2, b2,b3,W3 = model['W1'], model['b1'], model['W2'], model['b2'],model['b3'],model[\"W3\"]\n",
    "    \n",
    "    # Update parameters\n",
    "    W1 -= learning_rate * grads['dW1']\n",
    "    b1 -= learning_rate * grads['db1']\n",
    "    W2 -= learning_rate * grads['dW2']\n",
    "    b2 -= learning_rate * grads['db2']\n",
    "    W3 -= learning_rate * grads['dW3']\n",
    "    b3 -= learning_rate * grads['db3']\n",
    "    \n",
    "    # Store and return parameters\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3':W3,'b3':b3}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x):\n",
    "    # Do forward pass\n",
    "    c = forward_prop(model,x)\n",
    "    #get y_hat\n",
    "    y_hat = np.argmax(c['a3'], axis=1)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(model,x,y):\n",
    "    # Get total number of examples\n",
    "    m = y.shape[0]\n",
    "    # Do a prediction with the model\n",
    "    pred = predict(model,x)\n",
    "    # Ensure prediction and truth vector y have the same shape\n",
    "    pred = pred.reshape(y.shape)\n",
    "    # Calculate the number of wrong examples\n",
    "    error = np.sum(np.abs(pred-y))\n",
    "    # Calculate accuracy\n",
    "    return (m - error)/m * 100\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,X_,y_,learning_rate, epochs=20000, print_loss=False):\n",
    "    # Gradient descent. Loop over epochs\n",
    "    for i in range(0, epochs):\n",
    "\n",
    "        # Forward propagation\n",
    "        cache = forward_prop(model,X_)\n",
    "        #a1, probs = cache['a1'],cache['a2']\n",
    "        # Backpropagation\n",
    "        \n",
    "        grads = backward_prop(model,cache,y_)\n",
    "        # Gradient descent parameter update\n",
    "        # Assign new parameters to the model\n",
    "        model = update_parameters(model=model,grads=grads,learning_rate=learning_rate)\n",
    "    \n",
    "        # Pring loss & accuracy every 100 iterations\n",
    "        if print_loss and i % 100 == 0:\n",
    "            a3 = cache['a3']\n",
    "            print('Loss after iteration',i,':',softmax_loss(y_,a3))\n",
    "            y_hat = predict(model,X_)\n",
    "            y_true = y_.argmax(axis=1)\n",
    "            print('Accuracy after iteration',i,':',accuracy_score(y_pred=y_hat,y_true=y_true)*100,'%')\n",
    "            losses.append(accuracy_score(y_pred=y_hat,y_true=y_true)*100)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0 : 1.4619703912286877\n",
      "Accuracy after iteration 0 : 24.539877300613497 %\n",
      "Loss after iteration 100 : 0.5410202294391878\n",
      "Accuracy after iteration 100 : 79.75460122699386 %\n",
      "Loss after iteration 200 : 0.4080293260109097\n",
      "Accuracy after iteration 200 : 85.88957055214725 %\n",
      "Loss after iteration 300 : 0.35777911648501554\n",
      "Accuracy after iteration 300 : 87.73006134969326 %\n",
      "Loss after iteration 400 : 0.3309813130264834\n",
      "Accuracy after iteration 400 : 88.95705521472392 %\n",
      "Loss after iteration 500 : 0.31100773668997195\n",
      "Accuracy after iteration 500 : 88.95705521472392 %\n",
      "Loss after iteration 600 : 0.29279721209183823\n",
      "Accuracy after iteration 600 : 88.95705521472392 %\n",
      "Loss after iteration 700 : 0.27377332699561086\n",
      "Accuracy after iteration 700 : 88.95705521472392 %\n",
      "Loss after iteration 800 : 0.26020299559480736\n",
      "Accuracy after iteration 800 : 88.95705521472392 %\n",
      "Loss after iteration 900 : 0.2451752446736815\n",
      "Accuracy after iteration 900 : 88.95705521472392 %\n",
      "Loss after iteration 1000 : 0.23328634720806354\n",
      "Accuracy after iteration 1000 : 91.41104294478528 %\n",
      "Loss after iteration 1100 : 0.22387338114330593\n",
      "Accuracy after iteration 1100 : 92.02453987730061 %\n",
      "Loss after iteration 1200 : 0.20024678639973392\n",
      "Accuracy after iteration 1200 : 92.02453987730061 %\n",
      "Loss after iteration 1300 : 0.18741428035754282\n",
      "Accuracy after iteration 1300 : 92.63803680981594 %\n",
      "Loss after iteration 1400 : 0.17602379298318693\n",
      "Accuracy after iteration 1400 : 93.25153374233128 %\n",
      "Loss after iteration 1500 : 0.16567932515430203\n",
      "Accuracy after iteration 1500 : 93.86503067484662 %\n",
      "Loss after iteration 1600 : 0.15773723847206594\n",
      "Accuracy after iteration 1600 : 94.47852760736197 %\n",
      "Loss after iteration 1700 : 0.15298508918378761\n",
      "Accuracy after iteration 1700 : 94.47852760736197 %\n",
      "Loss after iteration 1800 : 0.1496036648962879\n",
      "Accuracy after iteration 1800 : 94.47852760736197 %\n",
      "Loss after iteration 1900 : 0.14690541626691983\n",
      "Accuracy after iteration 1900 : 94.47852760736197 %\n",
      "Loss after iteration 2000 : 0.14466711126872134\n",
      "Accuracy after iteration 2000 : 94.47852760736197 %\n",
      "Loss after iteration 2100 : 0.14274632609613921\n",
      "Accuracy after iteration 2100 : 94.47852760736197 %\n",
      "Loss after iteration 2200 : 0.14105478988307757\n",
      "Accuracy after iteration 2200 : 94.47852760736197 %\n",
      "Loss after iteration 2300 : 0.139538837147013\n",
      "Accuracy after iteration 2300 : 94.47852760736197 %\n",
      "Loss after iteration 2400 : 0.1381590536089957\n",
      "Accuracy after iteration 2400 : 94.47852760736197 %\n",
      "Loss after iteration 2500 : 0.13688041530549844\n",
      "Accuracy after iteration 2500 : 94.47852760736197 %\n",
      "Loss after iteration 2600 : 0.13566256655753042\n",
      "Accuracy after iteration 2600 : 94.47852760736197 %\n",
      "Loss after iteration 2700 : 0.1344357411388349\n",
      "Accuracy after iteration 2700 : 94.47852760736197 %\n",
      "Loss after iteration 2800 : 0.1329813513262281\n",
      "Accuracy after iteration 2800 : 94.47852760736197 %\n",
      "Loss after iteration 2900 : 0.12962412680703445\n",
      "Accuracy after iteration 2900 : 94.47852760736197 %\n",
      "Loss after iteration 3000 : 0.12410879960302691\n",
      "Accuracy after iteration 3000 : 95.0920245398773 %\n",
      "Loss after iteration 3100 : 0.1215630993417392\n",
      "Accuracy after iteration 3100 : 95.0920245398773 %\n",
      "Loss after iteration 3200 : 0.11863561826902874\n",
      "Accuracy after iteration 3200 : 95.70552147239265 %\n",
      "Loss after iteration 3300 : 0.11463650516964635\n",
      "Accuracy after iteration 3300 : 96.31901840490798 %\n",
      "Loss after iteration 3400 : 0.1127888361105384\n",
      "Accuracy after iteration 3400 : 96.31901840490798 %\n",
      "Loss after iteration 3500 : 0.11146549404821418\n",
      "Accuracy after iteration 3500 : 96.31901840490798 %\n",
      "Loss after iteration 3600 : 0.11035854626516534\n",
      "Accuracy after iteration 3600 : 96.31901840490798 %\n",
      "Loss after iteration 3700 : 0.10934417606445253\n",
      "Accuracy after iteration 3700 : 96.31901840490798 %\n",
      "Loss after iteration 3800 : 0.10834087629699816\n",
      "Accuracy after iteration 3800 : 96.31901840490798 %\n",
      "Loss after iteration 3900 : 0.10724380811746845\n",
      "Accuracy after iteration 3900 : 96.31901840490798 %\n",
      "Loss after iteration 4000 : 0.10580252083347381\n",
      "Accuracy after iteration 4000 : 96.31901840490798 %\n",
      "Loss after iteration 4100 : 0.10296056666845388\n",
      "Accuracy after iteration 4100 : 96.31901840490798 %\n",
      "Loss after iteration 4200 : 0.08980522637673993\n",
      "Accuracy after iteration 4200 : 96.93251533742331 %\n",
      "Loss after iteration 4300 : 0.06712672950627403\n",
      "Accuracy after iteration 4300 : 98.15950920245399 %\n",
      "Loss after iteration 4400 : 0.06540956728635056\n",
      "Accuracy after iteration 4400 : 98.15950920245399 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x220852d7710>]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#need to make random start from a base of 0\n",
    "np.random.seed(0)\n",
    "#epochs is one full training forward and back propagation\n",
    "#LEarning rate is hoow rapidly the parameters change\n",
    "# This is what we return at the end\n",
    "model = initialise_parameters(nn_input_dim=13, nn_hdim= 5, nn_output_dim= 3)\n",
    "model = train(model,X,y,learning_rate=0.07,epochs=4500,print_loss=True)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels\n",
    "yTest = dfTest[['Cultivar 1', 'Cultivar 2', 'Cultivar 3']].values\n",
    "\n",
    "# Get inputs; we define our x and y here.\n",
    "XTest = dfTest.drop(['Cultivar 1', 'Cultivar 2', 'Cultivar 3'], axis = 1)\n",
    "XTest.shape, yTest.shape # Print shapes just to check\n",
    "XTest = XTest.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now test on our test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTest = predict(model,XTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the test answers data array into a 1d array that can be compared for measuring the accuracy\n",
    "answersArray = []\n",
    "for index, row in dfTest[['Cultivar 1', 'Cultivar 2', 'Cultivar 3']].iterrows():\n",
    "    #print(str(index) +\" : \"+str((row.values)))\n",
    "    finalValue = 0;\n",
    "    for idx, num in enumerate(row.values):\n",
    "        correctedNum = idx*num\n",
    "        finalValue += correctedNum\n",
    "    answersArray.append(finalValue)\n",
    "    #print(str(index) +\":\" +str(finalValue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 0, 2, 0, 2, 1, 0, 0, 2, 1, 1, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The predicted answers\n",
    "yTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 0, 2, 0, 2, 1, 0, 0, 2, 1, 1, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the real answers\n",
    "np.array(answersArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#runn an accuracy score between the predicted numbers and real answers.\n",
    "accuracy_score(answersArray, yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
